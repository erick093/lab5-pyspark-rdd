{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# download our dataset tiny-shakespeare.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘datasets’: File exists\r\n",
      "--2022-12-14 13:09:58--  https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘datasets/tiny-shakespeare.txt’\r\n",
      "\r\n",
      "tiny-shakespeare.tx 100%[===================>]   1.06M  1.10MB/s    in 1.0s    \r\n",
      "\r\n",
      "2022-12-14 13:10:00 (1.10 MB/s) - ‘datasets/tiny-shakespeare.txt’ saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir datasets\n",
    "!wget -P datasets/ https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PySpark\n",
    "\n",
    "Como se discutió en las clases, cada aplicación Spark tiene un controlador(driver) Spark. Es el programa que declara las transformaciones y acciones en los RDD de datos y envía dichas solicitudes al administrador del clúster. En realidad, el controlador es el programa que crea SparkContext, conectándose a un administrador de clúster determinado, como Spark Master, YARN u otros. Los ejecutores ejecutan código de usuario, ejecutan cálculos y pueden almacenar datos en caché para su aplicación. SparkContext creará un trabajo que se dividirá en etapas. Las etapas se dividen en tareas programadas por SparkContext en un ejecutor.\n",
    "\n",
    "Al iniciar PySpark con el comando pyspark o usar un cuaderno bien configurado (como este), SparkContext se crea automáticamente en la variable sc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<SparkContext master=local[*] appName=pyspark-shell>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 40000 lines in the file\n"
     ]
    }
   ],
   "source": [
    "input_file = sc.textFile(\"datasets/tiny-shakespeare.txt\")\n",
    "num_lines = input_file.count()\n",
    "print(f\"there are {num_lines} lines in the file\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working with the Apache Spark Web UI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La interfaz de usuario web de Apache Spark se puede utilizar para diseccionar la vida de la ejecución de un trabajo.\n",
    "\n",
    "* Verá información sobre las máquinas de trabajo de su clúster (cantidad de núcleos, memoria disponible, ...) y una lista de aplicaciones. Verá una aplicación en ejecución, correspondiente al shell pyspark conectado a su Jupyter Notebook. Haga clic en su ID de aplicación para inspeccionar todos los trabajos que ha ejecutado esta aplicación.\n",
    "* Llegará a una página general con un resumen de su aplicación de shell pyspark.\n",
    "Siga la interfaz de usuario de detalles de la aplicación.\n",
    "\n",
    "Si es la primera vez que ejecuta la celda Notebook con el trabajo simple de \"recuento de líneas\", verá un solo elemento en la lista de trabajos. Preste atención al nombre del trabajo: corresponde al nombre de la Acción que disparó el trabajo, es decir, la acción de conteo. En este nivel, solo ve un resumen de trabajo aproximado: su tiempo de envío, su duración, etc.\n",
    "\n",
    "* Haga clic en la descripción del trabajo. Llegará a una página con una gran cantidad de detalles sobre su trabajo, comenzando con sus etapas (recuerde, los trabajos están hechos de etapas, que a su vez están hechas de tareas).\n",
    "* Expanda las secciones sobre la línea de tiempo del evento y la visualización de DAG.\n",
    "* A continuación, haga clic en la etapa del trabajo con el mismo nombre que el recuento de acciones que especificamos en nuestro código"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preguntas\n",
    "Usando el Web UI de Spark, responda las siguientes preguntas:\n",
    "\n",
    "* ¿Cuántas tareas (tasks) se ejecutaron en esta etapa (stage)?\n",
    "* ¿Cuántos recursos se utilizaron para ejecutar esta etapa?\n",
    "* ¿Cuánto tiempo tomó esta etapa?\n",
    "* ¿Cuánto tiempo tomó la acción de conteo?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejercicios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 1: Contar palabras\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el siguiente ejemplo, estamos interesados en las 10 palabras principales en términos de frecuencia de aparición. Para hacerlo, usamos un pequeño archivo de texto como entrada y deseamos trazar la frecuencia de términos de esas 10 palabras principales usando Matplotlib.\n",
    "\n",
    "Primero, usando el método textFile de SparkContext sc, creamos un RDD de strings. Cada string en el RDD es representativa de una línea en el archivo de texto. De forma vaga, podemos pensar que el primer RDD es un RDD de líneas de texto.\n",
    "\n",
    "Debido a que trabajamos en el alcance de las palabras, tenemos que transformar una línea del RDD actual en múltiples palabras, cada palabra es un objeto del nuevo RDD. Esto se hace usando la función flatMap.\n",
    "\n",
    "Luego, una función de mapa transforma cada palabra en el RDD en una sola tupla con 2 componentes: la palabra en sí y la cuenta de 1. Como habrás adivinado, este es un PairRDD, donde cada objeto es un par key-value.\n",
    "\n",
    "Podemos aprovechar la función **reduceByKey** para sumar todas las frecuencias de la misma palabra. Ahora, cada elemento en el RDD tiene la forma de: (palabra, frecuencia_total). Para ordenar las palabras por frecuencia de ocurrencia, podemos usar muchos enfoques. Uno de las maneras más simples es intercambiar cada tupla de modo que la frecuencia se convierta en el key y usar la función sortByKey."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "words = sc.textFile(\"datasets/tiny-shakespeare.txt\").repartition(8)\\\n",
    "            .flatMap(lambda line: line.split(\" \"))\\\n",
    "            .map(lambda word: (word, 1))\\\n",
    "            .reduceByKey(lambda a, b: a + b)\\\n",
    "            .map(lambda x: (x[1], x[0]))\\\n",
    "            .sortByKey(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preguntas\n",
    "\n",
    "* ¿Cuál es el tamaño del archivo de entrada?\n",
    "* ¿En cuántos bloques se divide el archivo de entrada? ¿Cuál es el tamaño del bloque?\n",
    "* ¿Cuántas tareas(tasks) se ejecutan en paralelo?\n",
    "* ¿Cuántos trabajos (jobs) se iniciaron tras la ejecución del código anterior? ¿Por qué (tenga en cuenta que no hay acciones!)?\n",
    "* ¿Qué significa una etapa \"skipped\"?\n",
    "* ¿Cuál es el número de bytes shuffled? ¿Cómo se compara esto con el número de bytes de entrada?\n",
    "* ¿Cree que Spark está haciendo un buen trabajo al equilibrar la carga entre los trabajadores? ¿Qué puede salir mal con el equilibrio de carga?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora tomamos las 10 palabras principales. Para hacerlo, usamos la función take. Esta función es una acción, por lo que inicia un trabajo. El trabajo se ejecuta en paralelo en el clúster. La función take devuelve una lista de tuplas (frecuencia, palabra) en el driver. El driver es la máquina que ejecuta el código Python. En nuestro caso, el driver es el nodo maestro del clúster."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7241, ''), (5437, 'the'), (4403, 'I'), (3923, 'to'), (3678, 'and'), (3275, 'of'), (2677, 'my'), (2610, 'a'), (2130, 'you'), (2073, 'in')]\n"
     ]
    }
   ],
   "source": [
    "top10 = words.take(10)\n",
    "print(top10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**ATENCIÓN**: La accion de colectar puede ser problemática. De hecho, un RDD puede tener un tamaño muy grande (¡es por eso que se distribuyen en varias máquinas en primer lugar!) y, por lo tanto, ¡podría agotar la RAM disponible en la máquina que ejecuta el controlador!. Es por eso que es mejor usar la función take, que devuelve una lista de tamaño fijo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ejercicio 2: Flights\n",
    "\n",
    "Tenemos un archivo CSV con datos de vuelos. El archivo contiene 29 columnas, que son:\n",
    "\n",
    "* **Year**: 1987-2008\n",
    "* **Month**: 1-12\n",
    "* **DayofMonth**: 1-31\n",
    "* **DayOfWeek**: 1 (Monday) - 7 (Sunday)\n",
    "* **DepTime**: actual departure time (local, hhmm)\n",
    "* **CRSDepTime**: scheduled departure time (local, hhmm)\n",
    "* **ArrTime**: actual arrival time (local, hhmm)\n",
    "* **CRSArrTime**: scheduled arrival time (local, hhmm)\n",
    "* **UniqueCarrier**: unique carrier code\n",
    "* **FlightNum**: flight number\n",
    "* **TailNum**: plane tail number\n",
    "* **ActualElapsedTime**: in minutes\n",
    "* **CRSElapsedTime**: in minutes\n",
    "* **AirTime**: in minutes\n",
    "* **ArrDelay**: arrival delay, in minutes\n",
    "* **DepDelay**: departure delay, in minutes\n",
    "* **Origin**: origin IATA airport code\n",
    "* **Dest**: destination IATA airport code\n",
    "* **Distance**: in miles\n",
    "* **TaxiIn**: taxi in time, in minutes\n",
    "* **TaxiOut**: taxi out time in minutes\n",
    "* **Cancelled**: was the flight cancelled? (1 = yes)\n",
    "* **CancellationCode**: reason for cancellation (A = carrier, B = weather, C = NAS, D = security)\n",
    "* **Diverted**: 1 = yes, 0 = no\n",
    "* **CarrierDelay**: in minutes\n",
    "* **WeatherDelay**: in minutes\n",
    "* **NASDelay**: in minutes\n",
    "* **SecurityDelay**: in minutes\n",
    "* **LateAircraftDelay**: in minutes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ejercicio 2.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En este ejercicio, estamos interesados solo en las siguientes columnas:\n",
    "\n",
    "* **CRSDepTime**: scheduled departure time (local, hhmm)\n",
    "* **UniqueCarrier**: unique carrier code\n",
    "\n",
    "Asuma que un night flight es un vuelo que sale después de las 19:00.\n",
    "\n",
    "Responda las siguientes preguntas:\n",
    "\n",
    "* ¿Cuántos vuelos nocturnos hay en total?\n",
    "* ¿Cuántos vuelos nocturnos hay por cada aerolínea? Muestre los 5 primeros resultados en terminos de volumen de vuelos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ejercicio 2.2\n",
    "\n",
    "En este ejercicio, considerando las siguientes columnas:\n",
    "\n",
    "* **UniqueCarrier**: unique carrier code\n",
    "* **DepDelay**: departure delay, in minutes\n",
    "\n",
    "Se les pide responder las siguientes preguntas:\n",
    "* Calcular el retraso promedio por aerolínea.\n",
    "* Calcular el retraso promedio por aerolínea y por mes.\n",
    "* Calcular el retraso promedio por aerolínea y por día de la semana."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ejercicio 2.3\n",
    "\n",
    "En este ejercicio, considerando las siguientes columnas:\n",
    "\n",
    "* **UniqueCarrier**: unique carrier code, Columna 8\n",
    "* **Cancelled**: was the flight cancelled? (1 = yes), Columna 21\n",
    "* **CancelationCode**: reason for cancellation (A = carrier, B = weather, C = NAS, D = security), Columna 22\n",
    "\n",
    "Se les pide responder las siguientes preguntas:\n",
    "\n",
    "* ¿Cuantos vuelos fueron cancelados en 1987? y en 2008?\n",
    "\n",
    "Responder las preguntas siguientes utilizando el año 2008:\n",
    "\n",
    "* ¿Cuál es la razón más común de cancelación de vuelos?\n",
    "* Por cada aerolínea, ¿cuál es la razón más común de cancelación de vuelos? Muestre los 5 primeros resultados en terminos de volumen de vuelos.\n",
    "* ¿Cuantos vuelos fueron cancelados por cada aerolínea, por razones de mal tiempo? Muestre los 5 primeros resultados en terminos de volumen de vuelos.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" YOUR CODE HERE \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}